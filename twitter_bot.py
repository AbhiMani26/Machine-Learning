# -*- coding: utf-8 -*-
"""Twitter_Bot.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1lWx6NLMO5YRlhsvsPLFQnWQK7yt4hOrb
"""

# Install HuggingFace Transformers (only once)
!pip install transformers --quiet
!pip install datasets --quiet
!pip install transformers --quiet
!pip install kaggle --quiet
!pip install optuna --quiet

import torch
import torch.nn as nn
import torch.optim as optim
from transformers import DistilBertTokenizer, DistilBertModel
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import json
from google.colab import files
import pandas as pd
from sklearn.preprocessing import StandardScaler
from transformers import DistilBertTokenizerFast
import torch
import torch.nn as nn
from transformers import DistilBertModel
from torch.utils.data import Dataset, DataLoader
import torch
import optuna
from torch.utils.data import Subset
from sklearn.model_selection import train_test_split
import numpy as np
import torch
from torch.optim import AdamW
import torch.nn as nn
from torch.utils.data import DataLoader, Subset
import torch
import torch.nn as nn
from torch.optim import AdamW
from torch.utils.data import DataLoader, Subset
from sklearn.model_selection import train_test_split
from torch.utils.data import Subset
from google.colab import drive
drive.mount('/content/drive')
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(device)

files.upload()  # Upload kaggle.json here

!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json
# Download dataset from Kaggle
!kaggle datasets download -d marvinvanbo/twibot-20 --unzip

# Load a small part of train.json to inspect
with open("train.json", "r") as f:
    train_data = json.load(f)

# Check how many entries
print("Train users loaded:", len(train_data))

# Print the structure of the first entry nicely
#print(json.dumps(train_data[0], indent=2))

def clean_int(val):
    try:
        return int(str(val).strip())
    except:
        return 0

def parse_user(user_json):
    profile = user_json["profile"]

    return {
        "description": profile.get("description", "").strip(),
        "followers_count": clean_int(profile.get("followers_count", 0)),
        "friends_count": clean_int(profile.get("friends_count", 0)),
        "statuses_count": clean_int(profile.get("statuses_count", 0)),
        "like_count": clean_int(profile.get("favourites_count", 0)),
        "verified": 1 if str(profile.get("verified", "False")).strip().lower() == "true" else 0,
        "tweets": " ".join(user_json.get("tweet") or [][:5]),
        "label": int(user_json.get("label", "0").strip())
    }

# Apply parser to all users
parsed_users = [parse_user(u) for u in train_data]
df = pd.DataFrame(parsed_users)

# Preview the result
df.head()

# List of metadata columns to normalize
meta_cols = ['followers_count', 'friends_count', 'statuses_count', 'like_count', 'verified']

# Create scaler and transform
scaler = StandardScaler()
meta_features = scaler.fit_transform(df[meta_cols])

# Convert back to DataFrame for easy access
meta_df = pd.DataFrame(meta_features, columns=meta_cols)
meta_df.head()

# Load pretrained tokenizer
tokenizer = DistilBertTokenizerFast.from_pretrained("distilbert-base-uncased")

# Combine text: description + tweets
texts = (df['description'] + " " + df['tweets']).fillna("")

# Tokenize the text
encoded_inputs = tokenizer(
    texts.tolist(),
    padding=True,
    truncation=True,
    max_length=128,
    return_tensors="pt"
)

# Preview
print("Input IDs shape:", encoded_inputs['input_ids'].shape)
print("Attention mask shape:", encoded_inputs['attention_mask'].shape)

class DeeProBotPP(nn.Module):
    def __init__(self, meta_input_size, hidden_size=256, dropout=0.3, num_layers=2):
        super(DeeProBotPP, self).__init__()

        # BERT encoder for text
        self.bert = DistilBertModel.from_pretrained("distilbert-base-uncased")

        # Freeze BERT if you want faster training (optional)
        # for param in self.bert.parameters():
        #     param.requires_grad = False

        # MLP for metadata (5 input features → dense layers)
        self.meta_mlp = nn.Sequential(
            nn.Linear(meta_input_size, 16),
            nn.ReLU(),
            nn.Linear(16, 8),
            nn.ReLU()
        )

        # Final classifier: [BERT + metadata] → bot score
        self.classifier = nn.Sequential(
            nn.Linear(768 + 8, 64),  # 768 from BERT + 8 from metadata MLP
            nn.ReLU(),
            nn.Linear(64, 1),
            nn.Sigmoid()
        )

    def forward(self, input_ids, attention_mask, metadata):
        # Get BERT output (use CLS token hidden state)
        bert_output = self.bert(input_ids=input_ids, attention_mask=attention_mask)
        cls_embedding = bert_output.last_hidden_state[:, 0, :]  # (batch_size, 768)

        # Metadata through MLP
        meta_output = self.meta_mlp(metadata)  # (batch_size, 8)

        # Concatenate both
        combined = torch.cat((cls_embedding, meta_output), dim=1)  # (batch_size, 776)

        # Classifier → bot score
        return self.classifier(combined)

# Combine everything into a custom Dataset
class BotDataset(Dataset):
    def __init__(self, input_ids, attention_mask, metadata, labels):
        self.input_ids = input_ids
        self.attention_mask = attention_mask
        self.metadata = metadata
        self.labels = labels

    def __len__(self):
        return self.input_ids.shape[0]

    def __getitem__(self, idx):
        return {
            'input_ids': self.input_ids[idx],
            'attention_mask': self.attention_mask[idx],
            'metadata': self.metadata[idx],
            'label': self.labels[idx]
        }

# Convert metadata to torch tensor
meta_tensor = torch.tensor(meta_df.values, dtype=torch.float32)

# Convert labels
labels = torch.tensor(df['label'].values, dtype=torch.float32)

# Build dataset
dataset = BotDataset(
    encoded_inputs['input_ids'],
    encoded_inputs['attention_mask'],
    meta_tensor,
    labels
)

# Create dataloader
train_loader = DataLoader(dataset, batch_size=32, shuffle=True)

def objective(trial):
    # 1. Suggest hyperparameters
    learning_rate = trial.suggest_float("lr", 1e-6, 5e-4, log=True)
    batch_size = trial.suggest_categorical("batch_size", [16, 32, 64])

    # 2. Create dataloaders with suggested batch size
    train_loader = DataLoader(Subset(dataset, train_idx), batch_size=batch_size, shuffle=True)
    val_loader = DataLoader(Subset(dataset, val_idx), batch_size=batch_size)

    # 3. Init model
    model = DeeProBotPP(meta_input_size=meta_tensor.shape[1]).to(device)
    optimizer = AdamW(model.parameters(), lr=learning_rate)
    loss_fn = nn.BCELoss()

    # 4. Train for 3 short epochs (quick search)
    for epoch in range(3):
        model.train()
        for batch in train_loader:
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            metadata = batch['metadata'].to(device)
            labels_batch = batch['label'].unsqueeze(1).to(device)

            outputs = model(input_ids, attention_mask, metadata)
            loss = loss_fn(outputs, labels_batch)

            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

    # 5. Validation loss
    model.eval()
    total_val_loss = 0
    with torch.no_grad():
        for batch in val_loader:
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            metadata = batch['metadata'].to(device)
            labels_batch = batch['label'].unsqueeze(1).to(device)

            outputs = model(input_ids, attention_mask, metadata)
            loss = loss_fn(outputs, labels_batch)
            total_val_loss += loss.item()

    avg_val_loss = total_val_loss / len(val_loader)
    return avg_val_loss

study = optuna.create_study(direction="minimize")  # minimize val loss
study.optimize(objective, n_trials=20)  # Try 20 combinations

print("✅ Best trial:")
print("  Loss:", study.best_value)
print("  Params:", study.best_params)

def objective_model_params(trial):
    # Fixed hyperparams from best trial
    learning_rate = 0.0004941104567861634
    batch_size = 16

    # Parameters to be tuned now
    dropout = trial.suggest_float("dropout", 0.1, 0.5)
    hidden_size = trial.suggest_int("hidden_size", 64, 512, step=64)
    num_layers = trial.suggest_int("num_layers", 1, 4)
    weight_decay = trial.suggest_float("weight_decay", 1e-5, 1e-2, log=True)

    # Dataloaders using existing indices
    train_loader = DataLoader(Subset(dataset, train_idx), batch_size=batch_size, shuffle=True)
    val_loader = DataLoader(Subset(dataset, val_idx), batch_size=batch_size)

    # Define model
    model = DeeProBotPP(
        meta_input_size=meta_tensor.shape[1],
        hidden_size=hidden_size,
        dropout=dropout,
        num_layers=num_layers
    ).to(device)

    optimizer = AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)
    loss_fn = nn.BCELoss()

    # Training loop
    for epoch in range(3):
        model.train()
        for batch in train_loader:
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            metadata = batch['metadata'].to(device)
            labels = batch['label'].to(device)

            optimizer.zero_grad()
            outputs = model(input_ids, attention_mask, metadata)
            loss = loss_fn(outputs.squeeze(), labels)  # ✅ Match dimensions
            loss.backward()
            optimizer.step()

    # Validation loop
    model.eval()
    val_losses = []
    with torch.no_grad():
        for batch in val_loader:
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            metadata = batch['metadata'].to(device)
            labels = batch['label'].to(device)

            outputs = model(input_ids, attention_mask, metadata)
            loss = loss_fn(outputs.squeeze(), labels)  # ✅ Match dimensions
            val_losses.append(loss.item())

    return np.mean(val_losses)

from torch.optim import AdamW
import torch.nn as nn

study_arch = optuna.create_study(direction="minimize")
study_arch.optimize(objective_model_params, n_trials=10)

print("✅ Best architecture:")
print("  Loss:", study_arch.best_value)
print("  Params:", study_arch.best_params)

# Make sure 'df' is your main user dataframe
train_idx, val_idx = train_test_split(
    np.arange(len(df)),
    test_size=0.2,
    random_state=42,
    stratify=df['label']  # Ensures balance of bots/non-bots in both sets
)

# Use GPU if available, else fallback to CPU
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("Running on:", device)

# Use GPU if available
print("Training on:", device)

# === Final Hyperparameters (based on Optuna tuning) ===
learning_rate = 0.0004941104567861634
batch_size = 16
dropout = 0.3694895802413003
hidden_size = 320
num_layers = 4
weight_decay = 0.006173331549784616
epochs = 10  # You can increase to 15 or 20 for better generalization

# === Final Model ===
model = DeeProBotPP(
    meta_input_size=meta_tensor.shape[1],
    hidden_size=hidden_size,
    dropout=dropout,
    num_layers=num_layers
).to(device)

# === Loss & Optimizer ===
loss_fn = nn.BCELoss()
optimizer = AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)

# === Dataloaders ===
train_loader = DataLoader(Subset(dataset, train_idx), batch_size=batch_size, shuffle=True)
val_loader = DataLoader(Subset(dataset, val_idx), batch_size=batch_size)

# === Training Loop ===
for epoch in range(epochs):
    model.train()
    total_train_loss = 0.0

    for batch in train_loader:
        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        metadata = batch['metadata'].to(device)
        labels = batch['label'].unsqueeze(1).to(device)  # Shape: [B, 1]

        optimizer.zero_grad()
        outputs = model(input_ids, attention_mask, metadata)
        loss = loss_fn(outputs, labels)
        loss.backward()
        optimizer.step()
        total_train_loss += loss.item()

    avg_train_loss = total_train_loss / len(train_loader)

    # === Validation ===
    model.eval()
    total_val_loss = 0.0
    with torch.no_grad():
        for batch in val_loader:
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            metadata = batch['metadata'].to(device)
            labels = batch['label'].unsqueeze(1).to(device)

            outputs = model(input_ids, attention_mask, metadata)
            loss = loss_fn(outputs, labels)
            total_val_loss += loss.item()

    avg_val_loss = total_val_loss / len(val_loader)

    print(f"Epoch {epoch+1}/{epochs} | Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f}")

# === Save Model ===
torch.save(model.state_dict(), "deeprobot_dd_final_v2.pt")
torch.save(model.state_dict(), "/content/drive/MyDrive/deeprobot_dd_final_v2.pt")
print("✅ Model saved as deeprobot_dd_final_v2.pt")

# Split again, this time taking a chunk from the full df for testing
train_val_idx, test_idx = train_test_split(
    np.arange(len(df)),
    test_size=0.1,
    random_state=42,
    stratify=df['label']
)

test_dataset = Subset(dataset, test_idx)
test_loader = DataLoader(test_dataset, batch_size=16)

# Step 1: Recreate the same model architecture
model = DeeProBotPP(
    meta_input_size=meta_tensor.shape[1],
    hidden_size=320,                  # From best trial
    dropout=0.3694895802413003,       # From best trial
    num_layers=4                      # From best trial
).to(device)

# Step 2: Load the saved weights
model.load_state_dict(torch.load("/content/drive/MyDrive/deeprobot_dd_final_v2.pt"))


# Step 3: Set to evaluation mode
model.eval()

with open("test.json", "r") as f:
    test_data = json.load(f)

parsed_test = [parse_user(user) for user in test_data]
test_df = pd.DataFrame(parsed_test)
test_meta = scaler.transform(test_df[meta_cols])
test_meta_tensor = torch.tensor(test_meta, dtype=torch.float32)
test_texts = (test_df['description'] + " " + test_df['tweets']).fillna("")

test_encoded = tokenizer(
    test_texts.tolist(),
    padding=True,
    truncation=True,
    max_length=128,
    return_tensors="pt"
)
test_labels = torch.tensor(test_df['label'].values, dtype=torch.float32)

test_dataset = BotDataset(
    test_encoded['input_ids'],
    test_encoded['attention_mask'],
    test_meta_tensor,
    test_labels
)

test_loader = DataLoader(test_dataset, batch_size=16)

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix

model.eval()
all_preds, all_labels = [], []

with torch.no_grad():
    for batch in test_loader:
        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        metadata = batch['metadata'].to(device)
        labels = batch['label'].to(device)

        outputs = model(input_ids, attention_mask, metadata)
        preds = (outputs > 0.70).float()

        all_preds.extend(preds.cpu().numpy())
        all_labels.extend(labels.cpu().numpy())

print("✅ Test Set Evaluation")
print("Accuracy:", accuracy_score(all_labels, all_preds))
print("F1 Score:", f1_score(all_labels, all_preds))
print("Report:\n", classification_report(all_labels, all_preds))
print("Confusion Matrix:\n", confusion_matrix(all_labels, all_preds))

model.eval()

true_labels = []
probs = []

with torch.no_grad():
    for batch in test_loader:
        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        metadata = batch['metadata'].to(device)
        labels = batch['label'].to(device)

        outputs = model(input_ids, attention_mask, metadata).squeeze(1)
        true_labels.extend(labels.cpu().numpy())
        probs.extend(outputs.cpu().numpy())

from sklearn.metrics import (
    precision_recall_curve, roc_curve, roc_auc_score,
    confusion_matrix, ConfusionMatrixDisplay,
    f1_score, precision_score, recall_score
)
import matplotlib.pyplot as plt
import numpy as np

# === 1. Precision-Recall Curve ===
precision, recall, _ = precision_recall_curve(true_labels, probs)
plt.figure(figsize=(6, 4))
plt.plot(recall, precision, marker='.')
plt.title("Precision-Recall Curve")
plt.xlabel("Recall")
plt.ylabel("Precision")
plt.grid(True)
plt.show()

# === 2. ROC Curve ===
fpr, tpr, _ = roc_curve(true_labels, probs)
roc_auc = roc_auc_score(true_labels, probs)
plt.figure(figsize=(6, 4))
plt.plot(fpr, tpr, label=f"AUC = {roc_auc:.2f}")
plt.plot([0, 1], [0, 1], linestyle="--", color='gray')
plt.title("ROC Curve")
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.legend()
plt.grid(True)
plt.show()

# === 3. Confusion Matrix (threshold = 0.7) ===
threshold = 0.7
preds = (np.array(probs) > threshold).astype(int)
cm = confusion_matrix(true_labels, preds)
disp = ConfusionMatrixDisplay(confusion_matrix=cm)
disp.plot(cmap='Blues')
plt.title(f"Confusion Matrix (Threshold = {threshold})")
plt.grid(False)
plt.show()

# === 4. Threshold Sweep Plot ===
thresholds = np.linspace(0.1, 0.9, 100)
f1s, precisions, recalls = [], [], []

for t in thresholds:
    temp_preds = (np.array(probs) > t).astype(int)
    f1s.append(f1_score(true_labels, temp_preds))
    precisions.append(precision_score(true_labels, temp_preds, zero_division=0))
    recalls.append(recall_score(true_labels, temp_preds))

plt.figure(figsize=(8, 5))
plt.plot(thresholds, f1s, label="F1 Score")
plt.plot(thresholds, precisions, label="Precision")
plt.plot(thresholds, recalls, label="Recall")
plt.xlabel("Threshold")
plt.ylabel("Score")
plt.title("Threshold Sweep")
plt.legend()
plt.grid(True)
plt.show()